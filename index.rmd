---
title: "Human Activity Recognition Prediction Model"
author: "tbudd"
date: "January 26, 2016"
output: html_document
---
##Overview
This project will use [Human Activity Recognition Project](http://groupware.les.inf.puc-rio.br/har) data to predict how well the Unilateral Dumbell Biceps Curl was performed by six individuals using activity monitors. 

The classification (column classe) of curl performance is:

* Exactly according to the specification (Class A)

* Throwing the elbows to the front (Class B)

* Lifting the dumbbell only halfway (Class C) 

* Lowering the dumbbell only halfway (Class D) 

* Throwing the hips to the front (Class E).

We use the random forest classifier to train a model to predict classe from 58 variables, pared down from the original 159 variables as described in the Preprocessing section below.

The random forest classifier was chosen because it has many advantages compared with other algorithms. For the purposes of this project the most important advantages are:

* It is one of the most accurate learning algoritms available (the final model had an accuracy of .9996, giving an out-of-sample error of .0004)

* It can handle thousands of input variables (there are more than 100 variables in the dataset)

* It does not require data preprocessing (no need for scaling or data transformations)

* It is resistant to outliers

We used 10-fold cross-validation to partition the training dataset. 

From [Wikipedia](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29):

> The advantage of this method [k-fold cross-validation] over repeated random sub-sampling [...] is that all observations are used for both training and validation, and each observation is used for validation exactly once.

##Load Necessary Libraries
```{r}
library(caret)
library(YaleToolkit)
library(randomForest)
```

##Download Data Reproducibly
```{r}
## code to download the data for **reproducibility**
validate<-read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))
training<-read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))
```

##Preprocessing
Remove the row id column. Remove columns with many missing values. Remove columns with near-zero variance to reduce the probability of overfitting the model and to reduce processing time.
```{r}
trn1<-training[,-1] ## remove row id column
val1<-validate[,-1] ## remove row id column

## remove cols with many (19216) missing values
keep<-whatis(trn1)$missing<1000
trn2<-trn1[,keep] 
val2<-val1[,keep]

## remove cols with near-zero variance
nzv<-nearZeroVar(trn2)
trn3<-trn2[,-nzv]
val3<-val2[,-nzv]
```

##Train the Model
Use 10-fold cross-validation and the random forest classification method.
```{r}
set.seed(776655) ## use set.seed for **reproducibility**

## Use ten-fold cross-validation
fitControl <- trainControl(method="cv", number=10, allowParallel = TRUE) # define training control

# train the model using the random forest classification method
fit3 <- train(classe~., data=trn3, trControl=fitControl, method="rf")
save(fit3,file="fit3f10.rda") ## save model in case we do get a valid run, because it takes so long to run.
```

##Results
```{r}
## evaluate the suitability of this model, including accuracy and a confusion matrix that is based on comparing the modeled data to the held out folds.
fit3$resample
accuracy<-mean(fit3$resample$Accuracy) ## mean of the accuracy of all the folds
oob<-1-accuracy ## out of sample error

## Display results
confusionMatrix.train(fit3)
accuracy
oob

## Use the model to make predictions on the validation dataset.
predict(fit3, val3[,1:57])
```
The predicted accuracy of the model is .9996, giving an out-of-sample error of .0004. The predictions from this model were 100% accurate on the validation dataset consisting of 20 observations.